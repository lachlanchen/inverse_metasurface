{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVIRIS Fixed Shape Experiment\n",
    "\n",
    "This notebook implements the AVIRIS Fixed Shape Experiment, which extends the original AVIRIS compression model to:\n",
    "1. Record important shapes from Stage 1 (initial, lowest condition number, lowest test MSE)\n",
    "2. Run Stage 2 with fixed shapes from Stage 1, optimizing only the decoder\n",
    "3. Compare performance of different fixed shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVIRIS Fixed Shape Experiment - Example Notebook\n",
    "\n",
    "This notebook demonstrates how to use the streamlined AVIRIS Fixed Shape Experiment code.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our streamlined modules\n",
    "from aviris_fixed_shape_notebook import *\n",
    "from aviris_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "# Configure data parameters\n",
    "tile_size = 100  # Size of image tiles\n",
    "use_cache = 'cache_simple'  # Cache directory\n",
    "folder = 'all'  # Which folder to use (or 'all')\n",
    "\n",
    "# Load data\n",
    "tiles = process_and_load_data(\n",
    "    tile_size=tile_size,\n",
    "    use_cache=use_cache,\n",
    "    folder=folder\n",
    ")\n",
    "\n",
    "# Display basic information about the data\n",
    "print(f\"Loaded {tiles.shape[0]} tiles with shape {tiles.shape[1:]} (C×H×W)\")\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "# Define experiment parameters\n",
    "output_dir = create_timestamp_dir(\"aviris_experiment\")\n",
    "print(f\"Results will be saved to: {output_dir}\")\n",
    "\n",
    "# Model parameters\n",
    "latent_dim = 11\n",
    "model_type = 'cnn'  # Use 'awan' if available\n",
    "use_fsf = True\n",
    "shape2filter_path = \"outputs_three_stage_20250322_145925/stageA/shape2spec_stageA.pt\"\n",
    "filter2shape_path = \"outputs_three_stage_20250322_145925/stageC/spec2shape_stageC.pt\"\n",
    "filter_scale_factor = 50.0\n",
    "\n",
    "# Training parameters\n",
    "stage1_epochs = 10  # Use more epochs (e.g., 50) for better results\n",
    "stage2_epochs = 20  # Use more epochs (e.g., 100) for better results\n",
    "batch_size = 8\n",
    "encoder_lr = 1e-3\n",
    "decoder_lr = 5e-4\n",
    "min_snr = 10\n",
    "max_snr = 40\n",
    "\n",
    "## Option 1: Run Stage 1 Only\n",
    "\n",
    "# Run stage 1 to learn filter shapes\n",
    "stage1_results = train_stage1(\n",
    "    tiles=tiles,\n",
    "    latent_dim=latent_dim,\n",
    "    model_type=model_type,\n",
    "    use_fsf=use_fsf,\n",
    "    shape2filter_path=shape2filter_path,\n",
    "    filter2shape_path=filter2shape_path,\n",
    "    filter_scale_factor=filter_scale_factor,\n",
    "    epochs=stage1_epochs,\n",
    "    batch_size=batch_size,\n",
    "    encoder_lr=encoder_lr,\n",
    "    decoder_lr=decoder_lr,\n",
    "    min_snr=min_snr,\n",
    "    max_snr=max_snr,\n",
    "    output_dir=os.path.join(output_dir, \"stage1_only\")\n",
    ")\n",
    "\n",
    "# Get results from stage 1\n",
    "model = stage1_results['model']\n",
    "recorded_shapes = stage1_results['recorded_shapes']\n",
    "recorded_metrics = stage1_results['recorded_metrics']\n",
    "train_loader = stage1_results['train_loader']\n",
    "test_loader = stage1_results['test_loader']\n",
    "\n",
    "# Visualize the learned filter shapes\n",
    "for shape_name, shape in recorded_shapes.items():\n",
    "    print(f\"{shape_name} shape:\")\n",
    "    print(f\"  - Condition Number: {recorded_metrics[shape_name]['condition_number']:.4f}\")\n",
    "    print(f\"  - Test MSE: {recorded_metrics[shape_name]['test_mse']:.6f}\")\n",
    "\n",
    "# Visualize some reconstructions\n",
    "visualize_reconstruction(model, test_loader, device)\n",
    "\n",
    "## Option 2: Run Stage 2 with a Specific Shape\n",
    "\n",
    "# Pick one of the shapes from stage 1\n",
    "shape_name = 'lowest_test_mse'  # Options: 'initial', 'lowest_condition_number', 'lowest_test_mse', 'final'\n",
    "shape = recorded_shapes[shape_name]\n",
    "\n",
    "# Train with this fixed shape\n",
    "stage2_results = train_stage2(\n",
    "    shape_name=shape_name,\n",
    "    shape=shape,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    model_type=model_type,\n",
    "    filter_scale_factor=filter_scale_factor,\n",
    "    epochs=stage2_epochs,\n",
    "    decoder_lr=decoder_lr,\n",
    "    min_snr=min_snr,\n",
    "    max_snr=max_snr,\n",
    "    output_dir=os.path.join(output_dir, \"stage2_only\")\n",
    ")\n",
    "\n",
    "# Get results from stage 2\n",
    "fixed_shape_model = stage2_results['model']\n",
    "\n",
    "# Visualize reconstructions with the fixed shape model\n",
    "visualize_reconstruction(fixed_shape_model, test_loader, device)\n",
    "\n",
    "## Option 3: Run Both Stages at Once for All Shapes\n",
    "\n",
    "# Run all stages at once\n",
    "all_results = run_all_stages(\n",
    "    tiles=tiles,\n",
    "    latent_dim=latent_dim,\n",
    "    model_type=model_type,\n",
    "    use_fsf=use_fsf,\n",
    "    shape2filter_path=shape2filter_path,\n",
    "    filter2shape_path=filter2shape_path,\n",
    "    filter_scale_factor=filter_scale_factor,\n",
    "    stage1_epochs=stage1_epochs,\n",
    "    stage2_epochs=stage2_epochs,\n",
    "    batch_size=batch_size,\n",
    "    encoder_lr=encoder_lr,\n",
    "    decoder_lr=decoder_lr,\n",
    "    min_snr=min_snr,\n",
    "    max_snr=max_snr,\n",
    "    add_random_baseline=True,  # Add a random shape for comparison\n",
    "    output_dir=os.path.join(output_dir, \"all_stages\")\n",
    ")\n",
    "\n",
    "## Option 4: Skip Stage 1 and Load Existing Shapes\n",
    "\n",
    "# Skip stage 1 and load shapes from a directory\n",
    "load_shapes_dir = \"results_fixed_shape_awan_20250402_052223/recorded_shapes/\"\n",
    "\n",
    "# Load recorded shapes\n",
    "recorded_shapes, recorded_metrics = load_shapes_from_directory(load_shapes_dir)\n",
    "\n",
    "# Check loaded shapes\n",
    "for shape_name, shape in recorded_shapes.items():\n",
    "    print(f\"{shape_name} shape:\")\n",
    "    print(f\"  - Condition Number: {recorded_metrics[shape_name]['condition_number']:.4f}\")\n",
    "    print(f\"  - Test MSE: {recorded_metrics[shape_name]['test_mse']:.6f}\")\n",
    "\n",
    "# Create dataset for stage 2\n",
    "dataset = AvirisDataset(tiles)\n",
    "    \n",
    "# Split into train and test sets\n",
    "test_split = 0.2\n",
    "test_size = int(len(dataset) * test_split)\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Train with a loaded shape\n",
    "shape_name = 'lowest_test_mse'\n",
    "shape = recorded_shapes[shape_name]\n",
    "\n",
    "stage2_results = train_stage2(\n",
    "    shape_name=shape_name,\n",
    "    shape=shape,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    model_type=model_type,\n",
    "    filter_scale_factor=filter_scale_factor,\n",
    "    epochs=stage2_epochs,\n",
    "    decoder_lr=decoder_lr,\n",
    "    min_snr=min_snr,\n",
    "    max_snr=max_snr,\n",
    "    output_dir=os.path.join(output_dir, \"stage2_loaded\")\n",
    ")\n",
    "\n",
    "## Analysis and Visualization\n",
    "\n",
    "# Compare different shape results\n",
    "if hasattr(all_results, 'stage2_results') and 'shapes' in all_results['stage2_results']:\n",
    "    # Create comparison data\n",
    "    stage2_comparison = {}\n",
    "    \n",
    "    for shape_name, results in all_results['stage2_results']['shapes'].items():\n",
    "        stage2_comparison[shape_name] = {\n",
    "            'train_losses': results['train_losses'],\n",
    "            'test_losses': results['test_losses'],\n",
    "            'condition_number': all_results['recorded_metrics'][shape_name]['condition_number'] \n",
    "                if shape_name in all_results['recorded_metrics'] else None\n",
    "        }\n",
    "    \n",
    "    # Visualize comparison\n",
    "    compare_stage2_results(\n",
    "        stage2_comparison, \n",
    "        output_dir=os.path.join(output_dir, \"shape_comparison\")\n",
    "    )\n",
    "\n",
    "# Compare model performance metrics\n",
    "if 'model' in stage2_results and 'model' in stage1_results:\n",
    "    # Calculate model sizes\n",
    "    stage1_model_size = calculate_model_size(stage1_results['model'])\n",
    "    stage2_model_size = calculate_model_size(stage2_results['model'])\n",
    "    \n",
    "    print(f\"Stage 1 model parameters: {stage1_model_size:,}\")\n",
    "    print(f\"Stage 2 model parameters: {stage2_model_size:,}\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    stage1_times = measure_inference_time(stage1_results['model'], test_loader, device)\n",
    "    stage2_times = measure_inference_time(stage2_results['model'], test_loader, device)\n",
    "    \n",
    "    print(f\"Stage 1 inference time: {stage1_times['avg_time_ms']:.2f} ms\")\n",
    "    print(f\"Stage 2 inference time: {stage2_times['avg_time_ms']:.2f} ms\")\n",
    "\n",
    "print(\"Experiment completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
